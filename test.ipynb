{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from unet import UNet\n",
    "from cnn import CNN\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.ops as ops\n",
    "import cv2 as cv\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(num_classes=1, in_channels=3, depth=5, merge_mode='concat')\n",
    "unet.load_state_dict(torch.load(\"models/unet.pt\", weights_only=True))\n",
    "unet.to(device);\n",
    "unet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "directory = \"test\\\\\"\n",
    "for file in os.listdir(directory):\n",
    "    img = cv.imread(os.path.join(directory, file))\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    img = cv.resize(img, (224, 224))\n",
    "    images.append(img)\n",
    "images = np.stack(images)/255\n",
    "\n",
    "r = images[:, :, :, 0]\n",
    "g = images[:, :, :, 1]\n",
    "b = images[:, :, :, 2]\n",
    "\n",
    "r = (r - 0.485)/(0.229)\n",
    "g = (g - 0.456)/(0.224)\n",
    "b = (b - 0.406)/(0.225)\n",
    "\n",
    "images = np.stack([r, g, b], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = torch.tensor(images, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = []\n",
    "for i in range(test_images.shape[0]):\n",
    "\tm = torch.sigmoid(unet(test_images[i:i+1].permute(0, 3, 1, 2).to(device)).squeeze()).cpu().detach()\n",
    "\tm = torch.threshold(m, 0.1, 0)\n",
    "\tpoints = ops.masks_to_boxes(m.unsqueeze(0)).int().tolist()[0]\n",
    "\timg = test_images[i] * m.unsqueeze(2)\n",
    "\timg = img[points[1]:points[3], points[0]:points[2]]\n",
    "\timg = cv.resize(img.numpy(), (64, 64))\n",
    "\tcropped_images.append(img)\n",
    "images = np.stack(cropped_images)\n",
    "# np.save(\"cropped_test_set.npy\", cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "cnn.load_state_dict(torch.load(\"models/cnn.pt\", weights_only=True))\n",
    "cnn.to(device)\n",
    "cnn.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"test\")\n",
    "test_set = torch.tensor(images, dtype=torch.float32)\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_set.shape[0]):\n",
    "\tt = test_set[i:i+1].to(device)\n",
    "\tl = cnn(t.permute(0, 3, 1, 2))\n",
    "\tpredictions.append(torch.argmax(torch.softmax(l, dim=1)).item()+1)\n",
    "dictionary = []\n",
    "for i in range(len(files)):\n",
    "\tdictionary.append([files[i], predictions[i]])\n",
    "with open(\"submission.csv\", mode='w', newline='') as file:\n",
    "\twriter = csv.writer(file)\n",
    "\twriter.writerows(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
